---
title: "Econ 343 Ch 7 Part 1 R Lecture"
output:
  html_document:
    number_sections: no
    toc: yes
    toc_collapsed: yes
    toc_float: yes
    toc_levels: 3
  beamer_presentation: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
#
# ===>> SETUP - this r-chunk does not usually need to be changed by the student.
#
# The following packages cover all of the commands that will be used in this class,
# as well as the files from the Wooldridge textbook.
# 
# For any assignment, you simply start by making a copy of this file and add the R chunks and
# text as needed below this setup chunk. 
#
# Note that these packages have been carefully selected to create a toolkit that is ideally
# suited for economic and business analysis. You might find it useful to keep this template and
# set of packages for future data analysis projects.
#
library(readr)      # for reading in .csv files
library(tidyverse)  # data wrangling
library(dplyr)      # for filter and select commands. etc.
library(wooldridge) # datasets in the textbook
library(stats)      # basic statistic commands
library(DataExplorer) # nice commands to learn about a new data set
library(stargazer)  # nice regression output
library(ggformula)  # easy graphing interface for ggplot
library(mosaic)     # accompanies ggformula
library(expss)      # apply_labels() cro()
library(scales)     # to format inline r as currency
library(AER)        # data and commands specific to econometrics
library(car)        # for linearhypothesis tests lht(), predict() hccm() heterosced.
library(lmtest)     # post-reg tests coeftest()
library(effects)    # to plot effects of one regressor, holding all others constant
library(visualize)  # to visualize hypothesis test p-values
library(dynlm)      # lm commands for time series data
library(plm)        # lm commands for panel data

options(digits=4)   # limits number of decimals to 4 when printing
knitr::opts_chunk$set(echo = TRUE) # TRUE = commands in r chunks to be echoed in outfile
```

# Ch 7 R Lecture

### R commands reviewed in this lecture
- mutate()
- if_else()
- case_when()


# Key Concepts

- Creating dummy variables
- Using and interpreting dummy regressors
- Interaction terms between dummy variables
- Interaction terms between dummy and quantitative variables
- Creating dummy variables

# Creating dummy variables

### if_else() example

Create four dummy variables: 1) female athletes, 2) male athletes, 3) female non-athletes, and 4) male non-athletes.
```{r}
mydata <- gpa2

mydata <- mydata %>%   mutate(
  femath      = if_else(female == 1 & athlete == 1, 1, 0),
  maleath     = if_else(female == 0 & athlete == 1, 1, 0),
  femaleath   = if_else(female == 1 & athlete == 0, 1, 0),
  malenoath   = if_else(female == 0 & athlete == 0, 1, 0))

```

### case_when() example 1

Collapse a variable with several categories down to a single dummy variable. 

Create `veryhappy` to indicate people who chose "very happy".

```{r}
mydata <- happiness
as_tibble(happiness)

mydata <- mydata %>%   mutate(
  veryhappy = case_when(happy == "very happy" ~ 1,
                        happy != "very happy" ~ 0))

mydata <- mydata %>%   mutate(
  veryhappy1 = case_when(happy == "very happy" | happy == "pretty happy" ~ 1,
                        TRUE ~ 0))

cro(mydata$happy,mydata$veryhappy)

cro(mydata$happy,mydata$veryhappy1)

```

### case_when() example 2

Create a dummy variable based on the range of values of a quantitative variable.

Create `ecobuy` to indicate if a person bought eco-friendly apples.

```{r}
mydata <- apple
mydata <- mydata %>%
  mutate(ecobuy = case_when(ecolbs == 0 ~ 0,
                            ecolbs  > 0 ~ 1))
```



# Different intercepts by category

**Economic Question** Does a negative return on stock affect CEO salary?

```{r}
mydata <- ceosal1 %>% select(salary, sales, roe, ros)

# use mutate to create dummy variable
mydata <- mydata %>% 
  mutate(rosneg = case_when(ros < 0 ~ 1,
                            ros >= 0 ~ 0))
# run the regression
lm1 <- lm(log(salary) ~ log(sales) + roe + rosneg, data = mydata)
lm2 <- lm(log(salary) ~ log(sales) + roe + ros, data = mydata)

model  <- lm1
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

stargazer(lm1, lm2, type="text")

```


**Explanation** Remember that the intercept (*Constant*) represents the average *log(salary)* when *roe=0* and *ros=0*. The coefficient on the dummy variable adds to the intercept when *ros* takes on any negative value. So, for CEOs with a negative value for *ros*, their average *log(salary)* is
**$\hat\beta_0 + \hat\beta_{rosneg}$ = `r b["(Intercept)"]` + `r b["rosneg"]` = `r b["(Intercept)"]  +  b["rosneg"]`**.

Typically, only the coefficient on the dummy variable is reported and it is interpreted as
the difference in the average salary between CEOs with positive vs. negative *ros*:

**Write-up** Negative *ros* does have a statistically significant impact on CEO salaries. The coefficient on *rosneg* (**$\hat\beta_{rosneg}=$ `r b["rosneg"]`**) implies that if the CEOâ€™s firm had a negative return on its stock over the 1988 to 1990 period, the CEO salary will
be about **`r b["rosneg"]*100`**% lower on average, controlling for *sales* and *roe*.  The **p-value=`r pval["rosneg"]`** is significant at the 5% level.

# Multiple Categories

## One variable with multiple categories

Example: Status

- 1 = out of workforce

- 2 = unemployed

- 3 = employed


Create a dummy variable for two of the three status categories to use in the regression.

```{r}
mydata.1 <- alcohol

mydata.1 <- mydata.1 %>% mutate(
  olf   = if_else(status == 1, 1, 0),
  unemp = if_else(status == 2, 1, 0),
  emp   = if_else(status == 3, 1, 0)
)

lm.1 <- lm(abuse ~ olf + unemp + fathalc + educ + age + famsize, data = mydata.1)
stargazer(lm.1,type = "text")
```


The intercept in the regression is the intercept for the **base** (left out) status, which in this case is "employed." The coefficient on "olf"  represents the **difference** in alcohol abuse between people out of the labor force vs. employed people. The intercept on "unemp" is the difference between unemployed people and employed people, as well.

## Multiple variables and multiple categories 

In the above example, the multiple categories all had to do with one variable - labor force status. Another instance of multiple categories can occur by combining single variable categories together. For example, married and female are each single categories.

There are two approaches to using these kinds of multiple categories in regressions.

### Approach 1: Create separate, mutually exclusive categories

Example 7.6 Examines wage differences across four mutually exclusive groups:

- single men
- single women
- married men
- married women

Create dummies that represent three of the four categories, and put them in the regression. 

$$ \widehat{log(wage)} = 
\beta_0 + \beta_1 singfem + \beta_2 marrfem + \beta_2 marrmale + control \ variables + u $$

```{r}

mydata.mc <- wage1

# Create a dummy variable that represents each category

mydata.mc <- mydata.mc %>% mutate(
  marrmale   = if_else(married == 1 & female == 0, 1, 0),
  marrfem    = if_else(married == 1 & female == 1, 1, 0),
  singfem    = if_else(married == 0 & female == 1, 1, 0)
)

lm1.mc <- lm(log(wage) ~ marrmale + marrfem + singfem + educ +
               exper + I(exper^2) + tenure + I(tenure^2), data = mydata.mc)

bhat1 <- coef(lm1.mc)

```


### Approach 2: Examine differences by each categories separately

In this approach, the single category dummy variables are not used to create new dummy variables. Instead, they are entered into the regression separately and also as an interaction term.

$$ \widehat{log(wage)} = 
\beta_0 + \beta_1 female + \beta_2 married + \beta_2 married*female + control \ variables + u $$

```{r}


lm2.mc <- lm(log(wage) ~ married*female + educ +
               exper + I(exper^2) + tenure + I(tenure^2), data = mydata.mc)

stargazer(lm1.mc,lm2.mc,type = "text",
          intercept.bottom = FALSE,
          column.labels = c("Approach 1","Approach 2"),
          keep = c("Constant","marrmale","marrfem","singfem",
                   "married","female","married:female","educ"),
          order = c("Constant","marrmale","marrfem","singfem",
                   "married","female","married:female","educ"))


```

- Notice that either approach gives the same estimated coefficient for single men and single women. 

- What is different betwen the two approaches is the coefficient on the multiple category - married women. 

  - In Approach 1, the coefficient is the difference between married women and the base group - single men. This is the easiest approach when the economic question is about the difference between the categories and the base group. On average, married women earn `r abs(bhat1["marrfem"])` % less than single men.
  
  - In Approach 2, the coefficient on the interaction term represents the difference between being "married and female" vs. just being married and just being female. So to determine the difference between married and female and the base category (single men) all three coeffiecients on the dummy variables must be added together.

- Why use Approach 2 if you have to do math to get the intercept for married women? Because Approach 2 allows you to ask a different question. 

- As discussed in section 7-4a, this approach allows you to examine whether the **gender differential depends on marital status** (or equivalently, if the marriage premium differs by gender). 


# Different Slopes by category

**Economic question** Do years of education impact wages differently by gender? 

```{r}
mydata <- wage1

lm2  <- lm(log(wage) ~ female + educ + female*educ + 
             exper + I(exper^2) + 
             tenure + I(tenure^2),mydata)

stargazer(lm2,type="text")

model <- lm2
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

```

**Write-up** In the regression results,

- **$\hat\beta_{educ}$** is the slope on ***educ*** for men

- **$\hat\beta_{femaleXeduc}$** is the difference in the slope on ***educ*** for women 

For each additional year of education, men's wages change on average 
by **$\hat{\beta_{educ}}$ = `r b["educ"]`**. 

The difference in the slope for women is **$\hat{\beta_{female X educ}}$ = `r round(b["female:educ"],3)` (*se* = `r se["female:educ"]`)**.

However, as seen by the standard error, the difference in the slope for women is not statistically significant. 

# Use mean regressions with dummies

The lack of statistical significance for the coefficient on ***female*** is likely because this is the wage difference when **educ = 0**. That is not a reasonable value for education. 

When a value of "0" for the quantitative variable in the interaction term is not reasonable, it is common practice to run the regressions at the mean of the explanatory variable. 


```{r}
mydata <- wage1

lm3  <- lm(log(wage) ~ female + I(educ - mean(educ)) + I(female*(educ - mean(educ))) + 
             exper + I(exper^2) + 
             tenure + I(tenure^2),mydata)

stargazer(lm3,type="text")

model <- lm3
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

```

**Write-up** Subtracting the mean(educ) from ***educ*** and from ***educ*** in the interaction term, the intercepts are more meaningful and more precisely estimated.

- the **intercept** is the average wage for men who have **mean(educ)** years of education

- **$\hat\beta_{female}$** is the difference in average wages between men and women who have **mean(educ)** years of education

For male workers with **educ = `r mean(mydata$educ)` years** the average wage is  **$\hat{\beta_0}$ = `r b["(Intercept)"]`**.

The difference in average wages for women is: **$\hat\beta_{female}$ = `r  b["female"]`**, which is now strongly statistically significant.

The gender difference in the slope coefficient on ***educ*** is the roughly the same and is still not statistically significant. It does not appear that the slope on ***educ*** is different for men and women.

