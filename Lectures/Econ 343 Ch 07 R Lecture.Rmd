---
title: "Econ 343 Ch  7 R Lecture"
output:
  html_document:
    number_sections: no
    toc: yes
    toc_collapsed: yes
    toc_float: yes
    toc_levels: 3
  beamer_presentation: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
#
# ===>> SETUP - this r-chunk does not usually need to be changed by the student.
#
# The following packages cover all of the commands that will be used in this class,
# as well as the files from the Wooldridge textbook.
# 
# For any assignment, you simply start by making a copy of this file and add the R chunks and
# text as needed below this setup chunk. 
#
# Note that these packages have been carefully selected to create a toolkit that is ideally
# suited for economic and business analysis. You might find it useful to keep this template and
# set of packages for future data analysis projects.
#
library(readr)      # for reading in .csv files
library(tidyverse)  # data wrangling
library(dplyr)      # for filter and select commands. etc.
library(wooldridge) # datasets in the textbook
library(stats)      # basic statistic commands
library(DataExplorer) # nice commands to learn about a new data set
library(stargazer)  # nice regression output
library(ggformula)  # easy graphing interface for ggplot
library(mosaic)     # accompanies ggformula
library(expss)      # apply_labels() cro()
library(scales)     # to format inline r as currency
library(AER)        # data and commands specific to econometrics
library(car)        # for linearhypothesis tests lht(), predict() hccm() heterosced.
library(lmtest)     # post-reg tests coeftest()
library(effects)    # to plot effects of one regressor, holding all others constant
library(visualize)  # to visualize hypothesis test p-values
library(dynlm)      # lm commands for time series data
library(plm)        # lm commands for panel data

options(digits=4)   # limits number of decimals to 4 when printing
knitr::opts_chunk$set(echo = TRUE) # TRUE = commands in r chunks to be echoed in outfile
```

# Ch 7 R Lecture

### R commands reviewed in this lecture

`mydata <- mydata %>%`

`mutate( newvar = case_when(`

`oldvar == 0 ~ 0,`

`oldvar >  0 ~ 1))`


# Key Concepts

- Using and interpreting dummy regressors
- Using dummy variable as the dependent variable
  - _Linear Probability Model_ 
  - similar to using a log(y) as a dependent variable
- Interaction terms between dummy variables
- Interaction terms between dummy and quantitative variables
- Creating dummy variables


# Different intercepts by category

**Economic Question** Does a negative return on stock affect CEO salary?

```{r}
mydata <- ceosal1 %>% select(salary, sales, roe, ros)

# use mutate to create dummy variable
mydata <- mydata %>% 
  mutate(rosneg = case_when(ros < 0 ~ 1,
                            ros >= 0 ~ 0))
# run the regression
lm1 <- lm(log(salary) ~ log(sales) + roe + rosneg, data = mydata)

model  <- lm1
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

stargazer(lm1,type="text")

```


**Explanation** Remember that the intercept (*Constant*) represents the average *log(salary)* when *roe=0* and *ros=0*. The coefficient on the dummy variable adds to the intercept when *ros* takes on any negative value. So, for CEOs with a negative value for *ros*, their average *log(salary)* is
**$\hat\beta_0 + \hat\beta_{rosneg}$ = `r b["(Intercept)"]` + `r b["rosneg"]` = `r b["(Intercept)"]  +  b["rosneg"]`**.

Typically, only the coefficient on the dummy variable is reported and it is interpreted as
the difference in the average salary between CEOs with positive vs. negative *ros*:

**Write-up** Negative *ros* does have a statistically significant impact on CEO salaries. The coefficient on *rosneg* (**$\hat\beta_{rosneg}=$ `r b["rosneg"]`**) implies that if the CEO’s firm had a negative return on its stock over the 1988 to 1990 period, the CEO salary will
be about **`r b["rosneg"]*100`**% lower on average, controlling for *sales* and *roe*.  The **p-value=`r pval["rosneg"]`** is significant at the 5% level.



# Different Slopes by category

**Economic question** Do years of education impact wages differently by gender? 

```{r}
mydata <- wage1

lm2  <- lm(log(wage) ~ female + educ + female*educ + 
             exper + I(exper^2) + 
             tenure + I(tenure^2),mydata)

stargazer(lm2,type="text")

model <- lm2
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

```

**Write-up** In the regression results,

- **$\hat\beta_{educ}$** is the slope on ***educ*** for men

- **$\hat\beta_{femaleXeduc}$** is the difference in the slope on ***educ*** for women 

For each additional year of education, men's wages change on average 
by **$\hat{\beta_{educ}}$ = `r b["educ"]`**. 

The difference in the slope for women is **$\hat{\beta_{female X educ}}$ = `r round(b["female:educ"],3)` (*se* = `r se["female:educ"]`)**.

However, as seen by the standard error, the difference in the slope for women is not statistically significant. 

# Use mean regressions with dummies

The lack of statistical significance for the coefficient on ***female*** is likely because this is the wage difference when **educ = 0**. That is not a reasonable value for education. 

When a value of "0" for the quantitative variable in the interaction term is not reasonable, it is common practice to run the regressions at the mean of the explanatory variable. 


```{r}
mydata <- wage1

lm3  <- lm(log(wage) ~ female + I(educ - mean(educ)) + I(female*(educ - mean(educ))) + 
             exper + I(exper^2) + 
             tenure + I(tenure^2),mydata)

stargazer(lm3,type="text")

model <- lm3
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

```

**Write-up** Subtracting the mean(educ) from ***educ*** and from ***educ*** in the interaction term, the intercepts are more meaningful and more precisely estimated.

- the **intercept** is the average wage for men who have **mean(educ)** years of education

- **$\hat\beta_{female}$** is the difference in average wages between men and women who have **mean(educ)** years of education

For male workers with **educ = `r mean(mydata$educ)` years** the average wage is  **$\hat{\beta_0}$ = `r b["(Intercept)"]`**.

The difference in average wages for women is: **$\hat\beta_{female}$ = `r  b["female"]`**, which is now strongly statistically significant.

The gender difference in the slope coefficient on ***educ*** is the roughly the same and is still not statistically significant. It does not appear that the slope on ***educ*** is different for men and women.

# Linear Probability Model

A dummy dependent variable takes on the value of "0" or "1". However, the predicted values from
the regression represent a probability, somewhere between 0 and 1, of belonging to the category. Regression coefficients represent the change in percentage points of the probability of belonging to the category. In this sense, interpretation in LPM is similar to that of log(y) dependent variables.

Here is an example.

**Economic question**

**Economic Question** (From Ch 7 Computer Exercise C13) Does the price difference 
between eco- and non-eco-friendly apples affect whether consumers by eco-friendly apples or not?

1. Frequently, you will start by creating the dummy dependent variable by recoding a quantitative variable. 

```{r}
mydata <- apple
mydata <- mydata %>%
  mutate(ecobuy = case_when(ecolbs == 0 ~ 0,
                            ecolbs  > 0 ~ 1))
stargazer(mydata,type="text")
```

**REMINDER: The mean of a dummy variable is the percent of observations with a value of "1".**

In this sample, **62%** of families bought eco-friendly apples.

2. Run the LPM regression, using the dummy as the dependent variable. Save the results for
post-estimation computations.

```{r}
lm7.13 <- lm(ecobuy ~ ecoprc + regprc + faminc + hhsize + educ + age, data=mydata)
stargazer(lm7.13,type="text")

b <- coef(lm7.13)
```

**Write-up** The primary regressor we want to examine is price of eco- and non-eco apples. The coefficient on ***ecoprc*** (price of eco-friendly apples) 
is **$\hat\beta_{ecoprc}$ = `r b["ecoprc"]`** dollars.  This means that if the price eco-apples is
$0.10 higher than regular apples, then the probability of buying eco-labeled apples *falls* by
**.10 * $\hat\beta_{ecoprc} * 100$ = `r 0.10 * b["ecoprc"] * 100` percentage points**.

The coefficient on ***regprc*** (price of regular apples) 
is **$\hat\beta_{regprc}$ = `r b["regprc"]`** dollars. If ***regprc*** increases by **10 cents**, the probability of buying eco-labeled apples **increases** by **.10 * $\hat\beta_{regprc}$ * 100 = `r 0.10 * b["regprc"] * 100` percentage points**.  


# LPM: Check predicted probabilities

Note: In the previous write-up, we must assume that the predicted probabilities 
are not close to the boundaries of zero and one. The next step checks this assumption.

- predicted $\hat{y}$ are predicted probabilities 
  - one concern researchers have about LPM is that it allows invalid predictions (negative or greater than 1)
  - but, the predictions (*fitted()* values) can be checked
  - if there are no or only a few negative or greater than 1 fitted values there is generally no cause for concern

```{r}
# Get the "y-hats"
# Recode y-hats to be dummy y_hat (take on 0/1 values only)

mydata <- mydata %>%
  mutate(yhat  = fitted(lm7.13),
         yhat_dummy = case_when(yhat >= .5 ~ 1,
                                yhat  < .5 ~ 0))

range_yhat <- range(mydata$yhat)
gt1_yhat   <- count(mydata$yhat > 1)
cro_cpct(mydata$ecobuy, mydata$yhat_dummy)

range_yhat[1]
range_yhat[2]
gt1_yhat


                            
```

**Write-up** The fitted probabilities range from `r range_yhat[1]` to `r range_yhat[2]`, so none are negative.  
There are `r gt1_yhat` predicted probabilities above 1, which is not a source of concern with 660 observations.


# LPM Compare actual to predicted probabilities

**Evaluate the LPM model: How well does this model predict eco-buying behavior?**

```{r}
# create a variable that determines if the predicted y_hat was correct or not
mydata <- mydata %>%
  mutate(correct_pred = case_when(yhat_dummy == ecobuy ~ 1,
                                  yhat_dummy != ecobuy ~ 0))

# overall correct prediction rate
Pct_correct_pred <- mean(mydata$correct_pred)

# percent of ecobuyers correctly predicted
Pct_correct_pred_ecobuy <- mean(mydata$correct_pred[mydata$ecobuy == 1])

# percent of non-ecobuyers correctly predicted
Pct_correct_pred_noecobuy <- mean(mydata$correct_pred[mydata$ecobuy == 0])

Pct_correct_pred
Pct_correct_pred_ecobuy
Pct_correct_pred_noecobuy
                            
```

**Write-up** This model does a decent job at predicting eco-buying behavior. But, much better for predicting those who do buy eco-friendly apples, vs. those who won't.

- Use the standard prediction rule – $\hat{y}_{dummy}$ = 1 if $\hat{y}$ > or = .5 and zero otherwise.

- The overall percent correctly predicted is **`r Pct_correct_pred*100`%.**

- The percent correctly predicted for those who are ecobuyers is
**`r Pct_correct_pred_ecobuy*100`%**.   

- The fraction correctly predicted for non-ecobuyers is **`r Pct_correct_pred_noecobuy*100`**%.  

Thus, using the usual prediction rule, the model predicts which consumers will *buy* eco-labeled apples about twice as accurately as it predicts that consumers will not buy eco-apples. 



