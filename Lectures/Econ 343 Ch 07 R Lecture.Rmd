---
title: "Econ 343 Ch  7 R Lecture"
output:
  html_document:
    number_sections: no
    toc: yes
    toc_collapsed: yes
    toc_float: yes
    toc_levels: 3
  beamer_presentation: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
#
# ===>> SETUP - this r-chunk does not usually need to be changed by the student.
#
# The following packages cover all of the commands that will be used in this class,
# as well as the files from the Wooldridge textbook.
# 
# For any assignment, you simply start by making a copy of this file and add the R chunks and
# text as needed below this setup chunk. 
#
# Note that these packages have been carefully selected to create a toolkit that is ideally
# suited for economic and business analysis. You might find it useful to keep this template and
# set of packages for future data analysis projects.
#
library(readr)      # for reading in .csv files
library(tidyverse)  # data wrangling
library(dplyr)      # for filter and select commands. etc.
library(wooldridge) # datasets in the textbook
library(stats)      # basic statistic commands
library(DataExplorer) # nice commands to learn about a new data set
library(stargazer)  # nice regression output
library(ggformula)  # easy graphing interface for ggplot
library(mosaic)     # accompanies ggformula
library(expss)      # apply_labels() cro()
library(scales)     # to format inline r as currency
library(AER)        # data and commands specific to econometrics
library(car)        # for linearhypothesis tests lht(), predict() hccm() heterosced.
library(lmtest)     # post-reg tests coeftest()
library(effects)    # to plot effects of one regressor, holding all others constant
library(visualize)  # to visualize hypothesis test p-values
library(dynlm)      # lm commands for time series data
library(plm)        # lm commands for panel data

options(digits=4)   # limits number of decimals to 4 when printing
knitr::opts_chunk$set(echo = TRUE) # TRUE = commands in r chunks to be echoed in outfile
```

# Ch 7 R Lecture

### R commands reviewed in this lecture
- mutate()
- if_else()
- case_when()


# Key Concepts

- Creating dummy variables
- Using and interpreting dummy regressors
- Using dummy variable as the dependent variable
  - _Linear Probability Model_ 
  - similar to using a log(y) as a dependent variable
- Interaction terms between dummy variables
- Interaction terms between dummy and quantitative variables
- Creating dummy variables

# Creating dummy variables

### if_else() example

Create four dummy variables: 1) female athletes, 2) male athletes, 3) female non-athletes, and 4) male non-athletes.
```{r}
mydata <- gpa2

mydata <- mydata %>%   mutate(
  femath      = if_else(female == 1 & athlete == 1, 1, 0),
  maleath     = if_else(female == 0 & athlete == 1, 1, 0),
  femaleath   = if_else(female == 1 & athlete == 0, 1, 0),
  malenoath   = if_else(female == 0 & athlete == 0, 1, 0))

```

### case_when() example 1

Collapse a variable with several categories down to a single dummy variable. 

Create `veryhappy` to indicate people who chose "very happy".

```{r}
mydata <- happiness
as_tibble(happiness)

mydata <- mydata %>%   mutate(
  veryhappy = case_when(happy == "very happy" ~ 1,
                        happy != "very happy" ~ 0))

mydata <- mydata %>%   mutate(
  veryhappy1 = case_when(happy == "very happy" | happy == "pretty happy" ~ 1,
                        TRUE ~ 0))

cro(mydata$happy,mydata$veryhappy)

cro(mydata$happy,mydata$veryhappy1)

```

### case_when() example 2

Create a dummy variable based on the range of values of a quantitative variable.

Create `ecobuy` to indicate if a person bought eco-friendly apples.

```{r}
mydata <- apple
mydata <- mydata %>%
  mutate(ecobuy = case_when(ecolbs == 0 ~ 0,
                            ecolbs  > 0 ~ 1))
```



# Different intercepts by category

**Economic Question** Does a negative return on stock affect CEO salary?

```{r}
mydata <- ceosal1 %>% select(salary, sales, roe, ros)

# use mutate to create dummy variable
mydata <- mydata %>% 
  mutate(rosneg = case_when(ros < 0 ~ 1,
                            ros >= 0 ~ 0))
# run the regression
lm1 <- lm(log(salary) ~ log(sales) + roe + rosneg, data = mydata)
lm2 <- lm(log(salary) ~ log(sales) + roe + ros, data = mydata)

model  <- lm1
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

stargazer(lm1, lm2, type="text")

```


**Explanation** Remember that the intercept (*Constant*) represents the average *log(salary)* when *roe=0* and *ros=0*. The coefficient on the dummy variable adds to the intercept when *ros* takes on any negative value. So, for CEOs with a negative value for *ros*, their average *log(salary)* is
**$\hat\beta_0 + \hat\beta_{rosneg}$ = `r b["(Intercept)"]` + `r b["rosneg"]` = `r b["(Intercept)"]  +  b["rosneg"]`**.

Typically, only the coefficient on the dummy variable is reported and it is interpreted as
the difference in the average salary between CEOs with positive vs. negative *ros*:

**Write-up** Negative *ros* does have a statistically significant impact on CEO salaries. The coefficient on *rosneg* (**$\hat\beta_{rosneg}=$ `r b["rosneg"]`**) implies that if the CEOâ€™s firm had a negative return on its stock over the 1988 to 1990 period, the CEO salary will
be about **`r b["rosneg"]*100`**% lower on average, controlling for *sales* and *roe*.  The **p-value=`r pval["rosneg"]`** is significant at the 5% level.

# Multiple Categories

## One variable with multiple categories

Example: Status

- 1 = out of workforce

- 2 = unemployed

- 3 = employed


Create a dummy variable for two of the three status categories to use in the regression.

```{r}
mydata.1 <- alcohol

mydata.1 <- mydata.1 %>% mutate(
  olf   = if_else(status == 1, 1, 0),
  unemp = if_else(status == 2, 1, 0),
  emp   = if_else(status == 3, 1, 0)
)

lm.1 <- lm(abuse ~ olf + unemp + fathalc + educ + age + famsize, data = mydata.1)
stargazer(lm.1,type = "text")
```


The intercept in the regression is the intercept for the **base** (left out) status, which in this case is "employed." The coefficient on "olf"  represents the **difference** in alcohol abuse between people out of the labor force vs. employed people. The intercept on "unemp" is the difference between unemployed people and employed people, as well.

## Multiple variables and multiple categories 

In the above example, the multiple categories all had to do with one variable - labor force status. Another instance of multiple categories can occur by combining single variable categories together. For example, married and female are each single categories.

There are two approaches to using these kinds of multiple categories in regressions.

### Approach 1: Create separate, mutually exclusive categories

Example 7.6 Examines wage differences across four mutually exclusive groups:

- single men
- single women
- married men
- married women

Create dummies that represent three of the four categories, and put them in the regression. 

$$ \widehat{log(wage)} = 
\beta_0 + \beta_1 singfem + \beta_2 marrfem + \beta_2 marrmale + control \ variables + u $$

```{r}

mydata.mc <- wage1

# Create a dummy variable that represents each category

mydata.mc <- mydata.mc %>% mutate(
  marrmale   = if_else(married == 1 & female == 0, 1, 0),
  marrfem    = if_else(married == 1 & female == 1, 1, 0),
  singfem    = if_else(married == 0 & female == 1, 1, 0)
)

lm1.mc <- lm(log(wage) ~ marrmale + marrfem + singfem + educ +
               exper + I(exper^2) + tenure + I(tenure^2), data = mydata.mc)

bhat1 <- coef(lm1.mc)

```


### Approach 2: Examine differences by each categories separately

In this approach, the single category dummy variables are not used to create new dummy variables. Instead, they are entered into the regression separately and also as an interaction term.

$$ \widehat{log(wage)} = 
\beta_0 + \beta_1 female + \beta_2 married + \beta_2 married*female + control \ variables + u $$

```{r}


lm2.mc <- lm(log(wage) ~ married*female + educ +
               exper + I(exper^2) + tenure + I(tenure^2), data = mydata.mc)

stargazer(lm1.mc,lm2.mc,type = "text",
          intercept.bottom = FALSE,
          column.labels = c("Approach 1","Approach 2"),
          keep = c("Constant","marrmale","marrfem","singfem",
                   "married","female","married:female","educ"),
          order = c("Constant","marrmale","marrfem","singfem",
                   "married","female","married:female","educ"))


```

- Notice that either approach gives the same estimated coefficient for single men and single women. 

- What is different betwen the two approaches is the coefficient on the multiple category - married women. 

  - In Approach 1, the coefficient is the difference between married women and the base group - single men. This is the easiest approach when the economic question is about the difference between the categories and the base group. On average, married women earn `r abs(bhat1["marrfem"])` % less than single men.
  
  - In Approach 2, the coefficient on the interaction term represents the difference between being "married and female" vs. just being married and just being female. So to determine the difference between married and female and the base category (single men) all three coeffiecients on the dummy variables must be added together.

- Why use Approach 2 if you have to do math to get the intercept for married women? Because Approach 2 allows you to ask a different question. 

- As discussed in section 7-4a, this approach allows you to examine whether the **gender differential depends on marital status** (or equivalently, if the marriage premium differs by gender). 


# Different Slopes by category

**Economic question** Do years of education impact wages differently by gender? 

```{r}
mydata <- wage1

lm2  <- lm(log(wage) ~ female + educ + female*educ + 
             exper + I(exper^2) + 
             tenure + I(tenure^2),mydata)

stargazer(lm2,type="text")

model <- lm2
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

```

**Write-up** In the regression results,

- **$\hat\beta_{educ}$** is the slope on ***educ*** for men

- **$\hat\beta_{femaleXeduc}$** is the difference in the slope on ***educ*** for women 

For each additional year of education, men's wages change on average 
by **$\hat{\beta_{educ}}$ = `r b["educ"]`**. 

The difference in the slope for women is **$\hat{\beta_{female X educ}}$ = `r round(b["female:educ"],3)` (*se* = `r se["female:educ"]`)**.

However, as seen by the standard error, the difference in the slope for women is not statistically significant. 

# Use mean regressions with dummies

The lack of statistical significance for the coefficient on ***female*** is likely because this is the wage difference when **educ = 0**. That is not a reasonable value for education. 

When a value of "0" for the quantitative variable in the interaction term is not reasonable, it is common practice to run the regressions at the mean of the explanatory variable. 


```{r}
mydata <- wage1

lm3  <- lm(log(wage) ~ female + I(educ - mean(educ)) + I(female*(educ - mean(educ))) + 
             exper + I(exper^2) + 
             tenure + I(tenure^2),mydata)

stargazer(lm3,type="text")

model <- lm3
b      <- summary(model)$coef[,1]           # estimated coefficient (beta_hat)
se     <- summary(model)$coef[,2]           # se of the estimated coefficient
tstat  <- summary(model)$coef[,3]           # t-stat computed by r
pval   <- summary(model)$coef[,4]           # p-value for H0: beta = 0 (two-sided alternative)

```

**Write-up** Subtracting the mean(educ) from ***educ*** and from ***educ*** in the interaction term, the intercepts are more meaningful and more precisely estimated.

- the **intercept** is the average wage for men who have **mean(educ)** years of education

- **$\hat\beta_{female}$** is the difference in average wages between men and women who have **mean(educ)** years of education

For male workers with **educ = `r mean(mydata$educ)` years** the average wage is  **$\hat{\beta_0}$ = `r b["(Intercept)"]`**.

The difference in average wages for women is: **$\hat\beta_{female}$ = `r  b["female"]`**, which is now strongly statistically significant.

The gender difference in the slope coefficient on ***educ*** is the roughly the same and is still not statistically significant. It does not appear that the slope on ***educ*** is different for men and women.

# Linear Probability Model

A dummy dependent variable takes on the value of "0" or "1". However, the predicted values from
the regression represent a probability, somewhere between 0 and 1, of belonging to the category. Regression coefficients represent the change in percentage points of the probability of belonging to the category. In this sense, interpretation in LPM is similar to that of log(y) dependent variables.

Here is an example.

**Economic question**

**Economic Question** (From Ch 7 Computer Exercise C13) Does the price difference 
between eco- and non-eco-friendly apples affect whether consumers by eco-friendly apples or not?

1. Frequently, you will start by creating the dummy dependent variable by recoding a quantitative variable. 

```{r}
mydata <- apple
mydata <- mydata %>%
  mutate(ecobuy = case_when(ecolbs == 0 ~ 0,
                            ecolbs  > 0 ~ 1))
stargazer(mydata,type="text")
```

**REMINDER: The mean of a dummy variable is the percent of observations with a value of "1".**

In this sample, **62%** of families bought eco-friendly apples.

2. Run the LPM regression, using the dummy as the dependent variable. Save the results for
post-estimation computations.

```{r}
lm7.13 <- lm(ecobuy ~ ecoprc + regprc + faminc + hhsize + educ + age, data=mydata)
stargazer(lm7.13,type="text")

b <- coef(lm7.13)
```

**Write-up** The primary regressor we want to examine is price of eco- and non-eco apples. The coefficient on ***ecoprc*** (price of eco-friendly apples) 
is **$\hat\beta_{ecoprc}$ = `r b["ecoprc"]`** dollars.  This means that if the price eco-apples is
$0.10 higher than regular apples, then the probability of buying eco-labeled apples *falls* by
**.10 * $\hat\beta_{ecoprc} * 100$ = `r 0.10 * b["ecoprc"] * 100` percentage points**.

The coefficient on ***regprc*** (price of regular apples) 
is **$\hat\beta_{regprc}$ = `r b["regprc"]`** dollars. If ***regprc*** increases by **10 cents**, the probability of buying eco-labeled apples **increases** by **.10 * $\hat\beta_{regprc}$ * 100 = `r 0.10 * b["regprc"] * 100` percentage points**.  


# LPM: Check predicted probabilities

Note: In the previous write-up, we must assume that the predicted probabilities 
are not close to the boundaries of zero and one. The next step checks this assumption.

- predicted $\hat{y}$ are predicted probabilities 
  - one concern researchers have about LPM is that it allows invalid predictions (negative or greater than 1)
  - but, the predictions (*fitted()* values) can be checked
  - if there are no or only a few negative or greater than 1 fitted values there is generally no cause for concern

```{r}
# Get the "y-hats"
# Recode y-hats to be dummy y_hat (take on 0/1 values only)

mydata <- mydata %>%
  mutate(yhat  = fitted(lm7.13),
         yhat_dummy = case_when(yhat >= .5 ~ 1,
                                yhat  < .5 ~ 0))

range_yhat <- range(mydata$yhat)
gt1_yhat   <- count(mydata$yhat > 1)
cro_cpct(mydata$ecobuy, mydata$yhat_dummy)

range_yhat[1]
range_yhat[2]
gt1_yhat


                            
```

**Write-up** The fitted probabilities range from `r range_yhat[1]` to `r range_yhat[2]`, so none are negative.  
There are `r gt1_yhat` predicted probabilities above 1, which is not a source of concern with 660 observations.


# LPM Compare actual to predicted probabilities

**Evaluate the LPM model: How well does this model predict eco-buying behavior?**

```{r}
# create a variable that determines if the predicted y_hat was correct or not
mydata <- mydata %>%
  mutate(correct_pred = case_when(yhat_dummy == ecobuy ~ 1,
                                  yhat_dummy != ecobuy ~ 0))

# overall correct prediction rate
Pct_correct_pred <- mean(mydata$correct_pred)

# percent of ecobuyers correctly predicted
Pct_correct_pred_ecobuy <- mean(mydata$correct_pred[mydata$ecobuy == 1])

# percent of non-ecobuyers correctly predicted
Pct_correct_pred_noecobuy <- mean(mydata$correct_pred[mydata$ecobuy == 0])

Pct_correct_pred
Pct_correct_pred_ecobuy
Pct_correct_pred_noecobuy
                            
```

**Write-up** This model does a decent job at predicting eco-buying behavior. But, much better for predicting those who do buy eco-friendly apples, vs. those who won't.

- Use the standard prediction rule â€“ $\hat{y}_{dummy}$ = 1 if $\hat{y}$ > or = .5 and zero otherwise.

- The overall percent correctly predicted is **`r Pct_correct_pred*100`%.**

- The percent correctly predicted for those who are ecobuyers is
**`r Pct_correct_pred_ecobuy*100`%**.   

- The fraction correctly predicted for non-ecobuyers is **`r Pct_correct_pred_noecobuy*100`**%.  

Thus, using the usual prediction rule, the model predicts which consumers will *buy* eco-labeled apples about twice as accurately as it predicts that consumers will not buy eco-apples. 



