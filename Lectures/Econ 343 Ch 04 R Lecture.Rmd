---
title: "Econ 343 Ch 4 R Lecture"
output:
  pdf_document:
    toc: true
    toc_depth: 3
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
#
# ===>> SETUP - this r-chunk does not usually need to be changed by the student.
#
# The following packages cover all of the commands that will be used in this class,
# as well as the files from the Wooldridge textbook.
# 
# For any assignment, you simply start by making a copy of this file and add the R chunks and
# text as needed below this setup chunk. 
#
# Note that these packages have been carefully selected to create a toolkit that is ideally
# suited for economic and business analysis. You might find it useful to keep this template and
# set of packages for future data analysis projects.
#
library(readr)      # for reading in .csv files
library(tidyverse)  # data wrangling
library(dplyr)      # for filter and select commands. etc.
library(wooldridge) # datasets in the textbook
library(stats)      # basic statistic commands
library(DataExplorer) # nice commands to learn about a new data set
library(stargazer)  # nice regression output
library(ggformula)  # easy graphing interface for ggplot
library(mosaic)     # accompanies ggformula
library(expss)      # apply_labels() cro()
library(scales)     # to format inline r as currency
library(AER)        # data and commands specific to econometrics
library(car)        # for linearHypothesis(), predict() hccm() heterosced and qqPlot()
library(lmtest)     # post-reg tests coeftest()
library(effects)    # to plot effects of one regressor, holding all others constant
library(visualize)  # to visualize hypothesis test p-values
library(dynlm)      # lm commands for time series data
library(plm)        # lm commands for panel data

options(digits=4)   # limits number of decimals to 4 when printing
options(scipen=999) # prevents scientific notation
knitr::opts_chunk$set(echo = TRUE) # TRUE = commands in r chunks to be echoed in outfile
```
### R and Rmarkdown syntax introduced in this lecture**

- **`summary()`** to save the regression results
- **`pt()`** to obtain p-values from a probability distribution
- **`lht()`**  to conduct linear hypothesis tests


# Ch 4 - Inference

# Learning objectives:

- understand the CLM assumptions

- determine statistical significance of estimated coefficients

- conduct eight-step Hypothesis test

- save regression results using **summary(lm1)$...**

- conduct linear hypothesis tests **lht()**


# **Classical Linear Model** Assumptions

1. Model is linear *in parameters*.
2. The data are from a random sample.
3. The explanatory variables are not perfectly collinear.
4. The model does not omit any relevant variables. (Zero Conditional Mean)
5. The error terms have the same variance across the range of x's. (Homoskedasticity)
6. **New** Error term is normally distributed.** (Normality)

## Normality

- If a regression is run and saved in a variable such as: *lm1*, a "Q-Q" plot
of the **residuals** checks for the normality assumption.

- If the residuals are not normally distributed, and the sample size is small 
the results from the regression
will not be statistically valid. 

- However, we know from the *Central Limit Theorem*, 
that if the sample size is large enough (say $n > 120$), the error terms do not have to be
normally distributed.

- Researchers also plot **residuals to check** if **heteroskedasticity** might be a problem.

## Normal Q-Q Plot

Use the `qqPlot()` command to examine the normality assumption. It is in the R package **car**.

```{r}
# Examine the residuals plotted against a normal distribution

lm1 <- lm(mpg ~ hp, data=mtcars)
qqPlot(lm1)
```

## Homoskedasticity

Another problem that could invalidate hypothesis tests is the presence of heteroskedasticity.
To evaluate whether this might be a problem, plot the residuals from the regression
against the fitted values. Check to see that the width of the scatter of the residuals about
the line stays the same along the x-axis.

```{r}
# Plot the residuals against the predicted values. There should be
# no trend, and the variance should be even at all levels.

gf_point(lm1$residuals ~ lm1$fitted) %>% 
  gf_lm()

```


# Hypothesis Testing in brief

The basic steps for all hypothesis tests are the following. 

1. State the **null** and **alternative** hypotheses. For example:
$$H_0: \beta_1 = 0$$
$$H_A: \beta_1 \ne 0$$

2. Compute the appropriate **test statistic** (e.g. t-test, F-test). 

3. Find the **p-value** associated with the test statistic from the appropriate distribution. Be sure to double it if it is a two-sided test.

4. State the conclusion and discuss.
   - Reject $H_0$ if $p-value < \alpha$
   - Fail to reject $H_0$ if $p-value >= \alpha$
where $\alpha$ represents the **level of significance** (e.g. 0.05, 0.01, or 0.001).

# Save regression results

- The following r chunk expands the concept of saving estimated
coefficients (**bhats**) to saving more results 

- This R chunk is handy to copy and paste when needed. 
``` {r}
# ==> change "lm1" below to the name used to save the regression 
model  <- lm1   

# ==> the following variables are VECTORS
#   To obtain the bhat,se,tstat,pval for one coefficient
#   put the name of the x variable in double quotes and square brackets.
#   For example, bhat["hp"] contains the estimated coefficient on hp
#   and se["hp"] contains the standard error of the coefficient on hp.

bhat   <- summary(model)$coef[,1]           # all estimated coefficients
se     <- summary(model)$coef[,2]           # all se's for the coefficients
tstat  <- summary(model)$coef[,3]           # all t-stats for coefficients
pval   <- summary(model)$coef[,4]           # all p-values for coefficients


# ==> the following variables are single VALUES
r_sqr      <- summary(model)$r.squared
adj_r_sqr  <- summary(model)$adj.r.squared
sigma      <- summary(model)$sigma

fstat <- summary(model)$fstatistic["value"] # the F-stat
df1   <- summary(model)$fstatistic["numdf"] # the numerator df = k (# Xs)
df2   <- summary(model)$fstatistic["dendf"] # the denominator df = n-k-1

```

## Choosing Test Statistics

The key to the hypothesis test is the **test statistic**. The test
statistic is computed in such a way that it 
is from a known distribution,
such as the *t* (short for Student's t distribution) or 
the *F* distribtution. Most linear hypothesis tests use the F test.

To illustrate, calculate the test statistic for the regression
coefficients by hand and compare it to the test statistic
provided in the regression output.

```{r}
tstat_by_hand <- (bhat["hp"] - 0)/se["hp"]

tstat_from_reggression <- tstat["hp"]
```

- This example will compute the t-stat for the coefficient on *hp* from the 
regression $mpg=\beta_0 + \beta_1 hp + u$:.

- The test statistic computed by hand should
be the same as the *t-stat* provided by the regression output.

$$t = \frac{\hat{\beta_1} - 0}{se(\hat{\beta_1})} =
\frac{`r bhat["hp"] ` - 0}{`r se["hp"] `} = `r tstat_by_hand`$$


This test statistic is the same one provided in the regression results: the *t-stat* reported for $\hat\beta_1$ is `r  tstat["hp"] `.

## 1. Critical Value Approach

- Use `qt(significance level, df)` in R console to find critical value
- Compare absolute value of test statistic to C.V. 
- Reject if |test statistics| > critical value.

Example:

Using a signficance level of .05, what is the critical value to compare the test statistic above against? 

1. Obtain the degrees of freedom, df = n-k-1. This is provided in the saved results from the regression above as **`df2`**. 

2. Use `qt(.05,df2)` in R to get the critical value

3. Test whether the absolute value of the tstat is greater than the absolute value of the critical value. If true, then REJECT!

```{r}
cv <- qt(.05,df2)

abs(tstat_by_hand) > abs(cv) 
```


## 2. P-value Approach

### Finding the p-value from the Student's t distribution

The value of the test statistic is placed on the *t* distribution and 
the probability that is in the tail is the **p-value**.

To determine the *p-value* for the test statistic use the R command **`pt()`**.

This command takes 2 argumments: 
- *tstat* = the test statistic
- *df* = degrees of freedom (df=n-k-1)

The degrees of freedom were saved from the regression results in the variable **`df2`**.

The command **pt(tstat,df)** returns the probability to
the **left** of the value *t-stat*. 

To get the probability in the upper tail, 
use a negative value for the t-stat by taking the absolute value and multiplying it by *-1*. 

Multiply the p-value by 2 for a two-sided t-test.

```{r}
alpha = .05 

pval_of_tstat_by_hand <- 2*pt(-abs(tstat_by_hand),df=df2) 

pval_of_tstat_by_hand < alpha

```

To verify, the *p-value* calculated by hand = `r pval_of_tstat_by_hand*100000` which
is the same *p-value*
provided by the regression output is `r pval["hp"]*100000` (both are scaled
by 100,000 for human readability).**


## Two-sided vs. One-sided Tests

- P-values from a one-sided test are the probability in one tail. 

- P-values from a two-sided test are the sum of the probabilities
in both tails. 

- P-values from standard regression output are from a two-sided test of
the null hypothesis :
$$H_0: \beta_j = 0 \\ H_A: \beta_j \ne 0$$ 

- A one-sided test is when $H_A$ is either greater than or less than:
$$H_0: \beta_j = 0 \\ H_A: \beta_j < 0$$ 

## Post-estimation (Linear Hypothesis) Tests 

Regression results automatically conduct the hypothesis
test to examine if coefficients are statistically significant. 
Additionally, regression results can be re-purposed to examine other questions.

Common post-estimation hypothesis tests include the following:

- $H_0: \beta_j = M$, to test if equal to some number other than 0
- $H_0: \beta_j = \beta_k$, to test whether two parameters are the same 
- $H_0: \beta_j = 2 * \beta_k$, to test whether one parameter is some function of another 
- $H_0: \beta_j = \beta_k = 0$, test whether a set of parameters are all 0 

# Example: Test against non-zero null hypothesis


This example shows how to construct a simple linear hypothesis test using the *lm1* regression results.

$H_0: \beta_j = 1$
$H_0: \beta_j \ne 1$

Suppose we want to conduct a two-sided test to see whether the coefficient is 1. 

- Let *M=1*. 
- Calculate the test statistic 
- Use R to obtain the p-value. 


```{r}
M <- 1  # <= set the value of for the null hypothesis here

tstat  <- (bhat["hp"] - M)/se["hp"]     # calculate *new* tstat
pval_1 <-   pt(-1*abs(tstat),df2) # obtain one-sided p-value
pval_2 <- 2*pt(-1*abs(tstat),df2) # obtain two-sided p-value
```


# Steps in reporting test Results

In the discussion of the results, it is important to be clear about every aspect of the test.

1. Write out the formal null and alternative hypotheses.

2. Indicate whehter the test is one- or two-sided, and justify.

3. Write out the formula for the test statistic

4. Insert the values from the regression results used into the formula

5. Report the one-sided or two-sided *p-value* of the test.  

## Example Write-up

To test the $H_0: \beta_1 = 1$ against the two-sided alternative, 
the following test statistic was computed:
$$t = \frac{\hat{\beta_1} - 1}{se(\hat{\beta_1})} =
\frac{`r bhat["hp"] ` - 1}{`r se["hp"] `} = `r tstat`$$.  

The *p-value* for the two-sided test against the null hypothesis is **$`r pval_2 `$** < $.001 \ (\alpha)$. 
The null hypothesis is rejected. There is strong evidence
to suggest that the coefficient is not equal to 1.



# 3 ways to obtain test statistic:

1. Calculate by hand
2. Use *lht()* command
3. Algebraic approach (textbook)

## Steps to calculate test statistics by hand

You can conduct a simple linear hypothesis test using the regression results as in the previous example.

Suppose we want to conduct a two-sided test to see whether $\beta_1$ = 1. 

- run the regression and save the estimates
- calculate the test statistic, for example: $\frac{\hat\beta_{x_1} - 1}{se(\hat\beta_{x_1})}$
- use the **pt()** command to obtain the p-value from the *Student's t distribution*
- double the p-value if it is a two-sided test
- compare the p-value to appropriate level of significance, say $\alpha = .05$.
- write up results


# How to use the lht() command  

From the *car* package,

Usage:

- **lht(model,"test")**


where "test" is a null hypothesis stated in terms of the names of the variables in 
the regression. 

## Use lht() to conduct *F-test* of **one** test (restriction)*

Example: To test if $\beta_{jc} = \beta_{college}$, use **"jc = univ"** in
the lht() command, as in the r chunk below.

```{r}
lm2     <- lm(lwage ~ jc + univ + exper,data=twoyear)
stargazer(lm2,type="text")
h1 <- lht(lm2,"jc = univ")

h1
```

- The test statistic (**F**) and the p-value (**Pr(>F)**) are found
in the last line of the output. 

- In the above example, the*
p-value is **`r h1$'Pr(>F)'[2]`**, and we fail to reject the null
hypothesis.

## Use lht() with **multiple restrictions**

Example: To test if $\beta_{jc} = 0$ and $\beta_{college} = 0$ jointly, use c("jc = 0","univ=0") in
the lht() command. 

- enclose each test separately in double quotes
- separate the tests by a **comma**
- enclose the list of tests within the combine command **c()**


```{r}
lm2     <- lm(lwage ~ jc + univ + exper,data=twoyear)
h2 <- linearHypothesis(lm2,c("jc = 0","univ=0"))
h2
```

- The test statistic (**F** = **`r round(h1$F[2],2)`**) and the p-value (**Pr(>F)** = **`r h2$'Pr(>F)'[2]`**) are found
in the last line of the output. 

We definitely reject this joint test.

### Test that **all** coefficients = 0 

Simply use the results from the F-test that came as a standard part of the regression output.

# How to interpret and report lht() results

1. Save the output from the linearHypothesis() command
2. Get the *F-stat, p-value, df1, and df2* from the saved output


```{r}

lm2     <- lm(lwage ~ jc + univ + exper,data=twoyear)
h1      <- lht(lm2,"jc=univ")

# ==> Save the pval, F-stat, and degrees of freedom from the lht() results 
pval    <- h1$'Pr(>F)'[2]
fstat   <- round(h1$F[2],2)
df1.h1  <- h1$Df[2]
df2.h1  <- h1$Res.Df[2]

```


## How to write-up hypothesis test results

In the discussion of the results, it is important to provide all relevant information.

1. Write out the **formal null and alternative hypotheses**.
2. Indicate whether the test is ***one- or two-sided**, and justify.
3. Identify the test statistic being computed.
4. Report the one-sided or two-sided *p-value* of the test. 
5. Interpret the findings.

### Example 1: Using hand calculated test statistic

To test the $H_0: \hat\beta_{hp} = 1$ against the two-sided alternative, 
the following test statistic was computed:
$$t = \frac{\hat{\beta_1} - 1}{se(\hat{\beta_1})} =
\frac{`r bhat["hp"] ` - 1}{`r se["hp"] `} = `r tstat`$$.  

### Example 2: Using linearHypothesis() test results

To test if $\beta_{jc} = \beta_{college}$ against a two-sided
alternative, an *F-test* was conducted. The resulting
test statistic is **`r fstat`** with df = (`r df1.h1`,`r df2.h1`). The 
p-value is **`r pval`**. Thus, the difference is **not** statistically significant. 
This sample does not provide
evidence that junior college and universities have different impacts on wages. 


# Appendix A: Use the algebraic approach to testing linear hypothesis

- This is the approach used in the textbook. It 
is useful to know how to do this. And the following steps
help clarify the textbook explanation.

- However, another approach is to use the **lht()** command in R. This
will be demonstrated in the next section.

- The algebraic approach is to create a variable that is the difference
in the two variables to be tested and run it in the regression 
so that the standard error on that variable computed by R is actually
the standard error on the difference between the two variables in the
hypothesis test.
 

Steps to test $\beta_1 - \beta_2 = 0$ using an algebraic approach:

1. Create a new variable that is the sum of the two variables. For example, 
create $totcoll = JC + Univ$. 

2. Run the regression with the new variable in place of with $\beta_2$.

3. Refer to the regression output. The test of the coefficient on the variable associated with $\beta_1$ will actually test the hypothesis of the difference in the two coefficients, $H_0:\beta_1 - \beta_2 = 0$.

Why this works:

- Let $\theta = \beta_1-\beta_2$
- Solve for $\beta_1$: $$\beta_1=\theta + \beta_2$$. 
- Plug this formula for $\beta_1$ in the original econometric model:
$$log(wage)=\beta_0 + (\theta + \beta_2) JC + \beta_2 (Univ) + u$$
- Collect terms:
$$log(wage)=\beta_0 + \theta JC + \beta_2 (JC + Univ) + u$$
- Replace $(JC + Univ)$ with the new variable: $totcoll = JC + Univ$ to get the model to be estimated:
$$log(wage)=\beta_0 + \theta JC + \beta_2 totcoll + u$$

### Try it out

```{r}
totcoll <- twoyear$jc + twoyear$univ

lm2     <- lm(lwage ~ jc + univ + exper,data=twoyear)
lm3     <- lm(lwage ~ jc + totcoll + exper,data=twoyear)

stargazer(lm2,lm3,type="text")

bhat2 <- coef(lm2)
```

The coefficient on *JC* is $\theta$ and $\theta = \beta_1 - \beta_2$. 

We tricked R into
calculating the standard error of the difference, and using it to test whether that 
difference is different from zero.

**Conclusion:**

- Each additional year of junior college increases 
the expected wage by $`r bhat2["jc"]*100`\%$

- Each additional year of university increases it by $`r bhat2["univ"]*100`\%$

- Both estimates are significant at $\alpha=.01$. 

- However, the difference between the the two coefficients is not statistically significant. 

- The
null hypothesis that the two types of schooling have equal impacts on wages fails to be rejected.


